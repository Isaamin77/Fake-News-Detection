# -*- coding: utf-8 -*-
"""FYP FAKE NEWS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15UNjN4Sh07IPVvU4lKEpDz-Dn1eioRRy
"""

from google.colab import files
import zipfile
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk

# Downloading necessary NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Uploading the zip file
uploaded = files.upload()

# Extracting the zip file
with zipfile.ZipFile("archive.zip", 'r') as zip_ref:
    zip_ref.extractall("/content")

# Listting extracted files
import os
print("Extracted files:", os.listdir("/content"))

# Loadding the datasets
fake_news = pd.read_csv("/content/Fake.csv")
real_news = pd.read_csv("/content/True.csv")

# Adding labels
fake_news['label'] = 0
real_news['label'] = 1

# Combining datasets
data = pd.concat([fake_news, real_news], ignore_index=True)

# Displaing dataset information
print(data.info())
print(data['label'].value_counts())

# Defining the preprocessing function
def preprocess_text(text):
    try:
        # Converted to lowercase
        text = text.lower()
        # Removing special characters numbers and whitespace
        text = re.sub(r'[^a-z\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        # manual tokenization
        words = text.split()
        # Removing stopwords
        stop_words = set(stopwords.words('english'))
        words = [word for word in words if word not in stop_words]
        # Lemmatizing words
        lemmatizer = WordNetLemmatizer()
        words = [lemmatizer.lemmatize(word) for word in words]
        # Joinning words back into a single string
        return ' '.join(words)
    except Exception as e:
        print(f"Error processing text: {text[:100]}")
        print(e)
        return ""

# Applying preprocessing to the text column
if 'text' in data.columns:
    data['processed_text'] = data['text'].apply(preprocess_text)
else:
    print("Error: Column 'text' not found in the dataset. Please check your column names.")

# Previewing the processed data
print(data.head())

# Saved the processed dataset to a new CSV file
data.to_csv("processed_data.csv", index=False)

print("Processed dataset saved as 'processed_data.csv'")

from sklearn.feature_extraction.text import TfidfVectorizer
import scipy.sparse

# Initializing the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,
    stop_words='english',
    ngram_range=(1, 2)
)

# Applying TF-IDF to the processed_text column
tfidf_features = tfidf_vectorizer.fit_transform(data['processed_text'])

# Convertting the TF-IDF matrix to a dense format and previewed the shape
print("TF-IDF Feature Matrix Shape:", tfidf_features.shape)

# Splitting data into features and labels
X = tfidf_features
y = data['label']

# Verifying shapes of X and y
print("Features Shape:", X.shape)
print("Labels Shape:", y.shape)

# Saving the TF-IDF matrix and labels to disk
scipy.sparse.save_npz("tfidf_features.npz", X)
data['label'].to_csv("labels.csv", index=False)

print("TF-IDF features saved as 'tfidf_features.npz' and labels as 'labels.csv'.")

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42  # 80% training, 20% testing
)

# Printting the shape of the training and testing sets
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Labels Shape:", y_train.shape)
print("Testing Labels Shape:", y_test.shape)

# Trainning a Support Vector Machine model
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

# Making predictions with the SVM model
svm_predictions = svm_model.predict(X_test)

# Evaluating the SVM model
print("SVM Model Performance:")
print("Accuracy:", accuracy_score(y_test, svm_predictions))
print("Precision:", precision_score(y_test, svm_predictions))
print("Recall:", recall_score(y_test, svm_predictions))
print("F1-Score:", f1_score(y_test, svm_predictions))

# Trainning a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Making predictions with the Random Forest model
rf_predictions = rf_model.predict(X_test)

# Evaluating the Random Forest model
print("\nRandom Forest Model Performance:")
print("Accuracy:", accuracy_score(y_test, rf_predictions))
print("Precision:", precision_score(y_test, rf_predictions))
print("Recall:", recall_score(y_test, rf_predictions))
print("F1-Score:", f1_score(y_test, rf_predictions))

# Displaying detailed classification reports
print("\nSVM Classification Report:")
print(classification_report(y_test, svm_predictions))

print("Random Forest Classification Report:")
print(classification_report(y_test, rf_predictions))

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np
import joblib

# Using a smaller subset of data for faster tuning
X_train_sample = X_train[:5000]
y_train_sample = y_train[:5000]


# Simplified parameter grid for SVM
svm_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear'],
}

# Performing GridSearchCV for SVM
svm_grid_search = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=2, scoring='f1', verbose=1, n_jobs=-1)
svm_grid_search.fit(X_train_sample, y_train_sample)

# Getting the best SVM model and parameters
best_svm_model = svm_grid_search.best_estimator_
print("Best SVM Parameters:", svm_grid_search.best_params_)

# Evaluating the tuned SVM model
svm_predictions_tuned = best_svm_model.predict(X_test)
print("\nTuned SVM Performance:")
print("Accuracy:", accuracy_score(y_test, svm_predictions_tuned))
print("Precision:", precision_score(y_test, svm_predictions_tuned))
print("Recall:", recall_score(y_test, svm_predictions_tuned))
print("F1-Score:", f1_score(y_test, svm_predictions_tuned))


# Simplified parameter grid for Random Forest
rf_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
}

# Performing RandomizedSearchCV for Random Forest
rf_random_search = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_param_grid,
                                      n_iter=5, cv=2, scoring='f1', verbose=1, n_jobs=-1, random_state=42)
rf_random_search.fit(X_train_sample, y_train_sample)

# Getting the best Random Forest model and parameters
best_rf_model = rf_random_search.best_estimator_
print("Best Random Forest Parameters:", rf_random_search.best_params_)

# Evaluatting the tuned Random Forest model
rf_predictions_tuned = best_rf_model.predict(X_test)
print("\nTuned Random Forest Performance:")
print("Accuracy:", accuracy_score(y_test, rf_predictions_tuned))
print("Precision:", precision_score(y_test, rf_predictions_tuned))
print("Recall:", recall_score(y_test, rf_predictions_tuned))
print("F1-Score:", f1_score(y_test, rf_predictions_tuned))


# SVM Confusion Matrix
print("\nConfusion Matrix for SVM:")
svm_cm = confusion_matrix(y_test, svm_predictions_tuned)
ConfusionMatrixDisplay(svm_cm, display_labels=["Fake", "Real"]).plot()

# Random Forest Confusion Matrix
print("\nConfusion Matrix for Random Forest:")
rf_cm = confusion_matrix(y_test, rf_predictions_tuned)
ConfusionMatrixDisplay(rf_cm, display_labels=["Fake", "Real"]).plot()

# Saving the TF-IDF vectorizer
joblib.dump(tfidf_vectorizer, "tfidf_vectorizer.pkl")


# Saving the best models using joblib
joblib.dump(best_svm_model, "best_svm_model.pkl")
joblib.dump(best_rf_model, "best_rf_model.pkl")

print("\nBest models saved as 'best_svm_model.pkl' and 'best_rf_model.pkl'.")
print("\nTF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'")

!pip install streamlit pyngrok

with open('app.py', 'w') as f:
    f.write("""\
import streamlit as st
import joblib
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk

# Downloadding required NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Loadding the trained model and TF-IDF vectorizer
model = joblib.load('best_rf_model.pkl')
tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')

# Defining text preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    words = text.split()

    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    return ' '.join(words)


st.title("Fake News Detection App")
st.write("Enter a news article below, and the app will predict whether it's **Real** or **Fake**.")

user_input = st.text_area("Enter News Article:", height=200)

if st.button("Predict"):
    if user_input:
        processed_input = preprocess_text(user_input)
        input_vector = tfidf_vectorizer.transform([processed_input])
        prediction = model.predict(input_vector)[0]

        if prediction == 1:
            st.success(" This news article is **Real**.")
        else:
            st.error(" This news article is **Fake**.")
    else:
        st.warning(" Please enter some text to make a prediction.")
""")

!ngrok authtoken 2sxGGgyPlRBm8K0lJcH203OVNwx_4kKm8z22PYTehKXRBnjyk

from pyngrok import ngrok

# Starting the Streamlit server
public_url = ngrok.connect(addr="8501")
print(" Access the Streamlit App here:", public_url)

!streamlit run app.py
